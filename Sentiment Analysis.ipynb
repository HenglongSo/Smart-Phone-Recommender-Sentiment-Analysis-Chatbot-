{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "#### IST664/CIS668 Nature Language Processing\n",
    "#### Sile Hu, Xiaobin Ning, Henglong So, Ziyun Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will first compare the accuracy of different features by using corss validation and use the feature which has the highest accuracy to do the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1. Accuracy Comparsion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we imported the packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/henglong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/henglong/nltk_data...\n",
      "[nltk_data] Error downloading 'averaged_perceptron_tagger' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-p\n",
      "[nltk_data]     ages/packages/taggers/averaged_perceptron_tagger.zip>:\n",
      "[nltk_data]     [Errno 54] Connection reset by peer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.sentiment\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /Users/henglong/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/henglong/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used the movie review sentences from the NLTK corpus to generate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentence_polarity.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences[:4]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie review sentences are not labeled individually, but can be retrieved by\n",
    "category. We create the list of documents where each document(sentence) is paired with its\n",
    "label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories()\n",
    "    for sent in sentence_polarity.sents(categories=cat)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the documents are in order by label, we mix them up for later separation into\n",
    "training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the set of words that will be used for features. This is essentially all the words in the entire document collection, except that we will limit it to the 2000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word, freq) in word_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we built our corss validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(num_folds, featuresets):\n",
    "    subset_size = len(featuresets)//num_folds\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i*subset_size:][:subset_size]\n",
    "        train_this_round = featuresets[:i*subset_size]+featuresets[(i+1)*subset_size:]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print(i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "        # find mean accuracy over all rounds\n",
    "        print('mean accuracy', sum(accuracy_list) / num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all these preliminary work, we generated BOW features, negation features, POS tag features and bigram features and compare the accuracy of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BOW features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the features for each document, using just the words, sometimes\n",
    "called the BOW or unigram features. The feature label will be ‘V_keyword’ for each\n",
    "keyword (aka word) in the word_features set, and the value of the feature will be\n",
    "Boolean, according to whether the word is contained in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test and run the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759\n",
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     19.8 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     16.3 : 1.0\n",
      "               V_generic = True              neg : pos    =     16.3 : 1.0\n",
      "                  V_dull = True              neg : pos    =     15.1 : 1.0\n",
      "                  V_flat = True              neg : pos    =     13.3 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.2 : 1.0\n",
      "             V_inventive = True              pos : neg    =     13.1 : 1.0\n",
      "               V_routine = True              neg : pos    =     12.9 : 1.0\n",
      "                    V_90 = True              neg : pos    =     12.3 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     12.3 : 1.0\n",
      "             V_realistic = True              pos : neg    =     11.7 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     11.7 : 1.0\n",
      "              V_provides = True              pos : neg    =     11.5 : 1.0\n",
      "                  V_warm = True              pos : neg    =     11.5 : 1.0\n",
      "              V_touching = True              pos : neg    =     11.3 : 1.0\n",
      "             V_absorbing = True              pos : neg    =     11.1 : 1.0\n",
      "                V_stupid = True              neg : pos    =     10.5 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     10.4 : 1.0\n",
      "                V_unless = True              neg : pos    =     10.3 : 1.0\n",
      "              V_mindless = True              neg : pos    =     10.3 : 1.0\n",
      "              V_chilling = True              pos : neg    =      9.7 : 1.0\n",
      "              V_powerful = True              pos : neg    =      9.7 : 1.0\n",
      "                 V_stale = True              neg : pos    =      9.6 : 1.0\n",
      "            V_apparently = True              neg : pos    =      9.6 : 1.0\n",
      "            V_meandering = True              neg : pos    =      9.6 : 1.0\n",
      "              V_captures = True              pos : neg    =      9.4 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.4 : 1.0\n",
      "                  V_loud = True              neg : pos    =      9.4 : 1.0\n",
      "                  V_ages = True              pos : neg    =      9.0 : 1.0\n",
      "           V_mesmerizing = True              pos : neg    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7570356472795498\n",
      "mean accuracy 0.07570356472795498\n",
      "1 0.7448405253283302\n",
      "mean accuracy 0.150187617260788\n",
      "2 0.7523452157598499\n",
      "mean accuracy 0.22542213883677298\n",
      "3 0.725140712945591\n",
      "mean accuracy 0.2979362101313321\n",
      "4 0.7692307692307693\n",
      "mean accuracy 0.374859287054409\n",
      "5 0.773921200750469\n",
      "mean accuracy 0.4522514071294559\n",
      "6 0.7317073170731707\n",
      "mean accuracy 0.5254221388367729\n",
      "7 0.7345215759849906\n",
      "mean accuracy 0.598874296435272\n",
      "8 0.7345215759849906\n",
      "mean accuracy 0.672326454033771\n",
      "9 0.7439024390243902\n",
      "mean accuracy 0.7467166979362101\n"
     ]
    }
   ],
   "source": [
    "cross_validation(10, featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of BOW feature is about 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Adding Negation Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation of opinions is an important part of opinion classification. Here we try a \n",
    "simple strategy. We look for negation words \"not\", \"never\" and \"no\" and negation\n",
    "that appears in contractions of the form \"doesn’t\".\n",
    "\n",
    "One strategy with negation words is to negate the word following the negation word,\n",
    "while other strategies negate all words up to the next punctuation or use syntax to find\n",
    "the scope of the negation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form of some of the words is a verb followed by n’t. Now in the Movie Review\n",
    "Corpus itself, the tokenization has these words all split into 3 words, e.g. “couldn”,\n",
    "“’”, and “t”. (and I have a NOT_features definition for this case). But in this\n",
    "sentence_polarity corpus, the tokenization keeps these forms of negation as one word\n",
    "ending in “n’t”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in list(sentences)[:50]:\n",
    "    for word in sent:\n",
    "        if (word.endswith(\"n't\")):\n",
    "            print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather',\n",
    "'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the feature set with all 2000 word features and 2000 Not word features set to\n",
    "false. If a negation occurs, add the following word as a Not word feature (if it’s in the\n",
    "top 2000 feature words), and otherwise add it as a regular feature word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = False\n",
    "        features['contains(NOT{})'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['contains(NOT{})'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['contains({})'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature sets as before, using the NOT_features extraction funtion, train the\n",
    "classifier and test the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "NOT_featuresets[0][0]['contains(NOTlike)']\n",
    "NOT_featuresets[0][0]['contains(always)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test and run the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765\n",
      "Most Informative Features\n",
      "    contains(engrossing) = True              pos : neg    =     21.7 : 1.0\n",
      "       contains(generic) = True              neg : pos    =     17.0 : 1.0\n",
      "      contains(mediocre) = True              neg : pos    =     17.0 : 1.0\n",
      "     contains(inventive) = True              pos : neg    =     15.7 : 1.0\n",
      "          contains(flat) = True              neg : pos    =     15.0 : 1.0\n",
      "       contains(routine) = True              neg : pos    =     15.0 : 1.0\n",
      "        contains(boring) = True              neg : pos    =     14.7 : 1.0\n",
      "    contains(refreshing) = True              pos : neg    =     14.3 : 1.0\n",
      "            contains(90) = True              neg : pos    =     13.0 : 1.0\n",
      "          contains(warm) = True              pos : neg    =     12.6 : 1.0\n",
      "     contains(wonderful) = True              pos : neg    =     12.6 : 1.0\n",
      "     contains(NOTenough) = True              neg : pos    =     12.3 : 1.0\n",
      "  contains(refreshingly) = True              pos : neg    =     12.3 : 1.0\n",
      "   contains(mesmerizing) = True              pos : neg    =     11.7 : 1.0\n",
      "     contains(realistic) = True              pos : neg    =     11.7 : 1.0\n",
      "      contains(provides) = True              pos : neg    =     11.4 : 1.0\n",
      "          contains(dull) = True              neg : pos    =     11.2 : 1.0\n",
      "      contains(mindless) = True              neg : pos    =     11.0 : 1.0\n",
      "        contains(stupid) = True              neg : pos    =     11.0 : 1.0\n",
      "         contains(stale) = True              neg : pos    =     11.0 : 1.0\n",
      "      contains(captures) = True              pos : neg    =     11.0 : 1.0\n",
      "        contains(unless) = True              neg : pos    =     10.3 : 1.0\n",
      "          contains(loud) = True              neg : pos    =      9.8 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      9.8 : 1.0\n",
      "    contains(meandering) = True              neg : pos    =      9.7 : 1.0\n",
      "    contains(apparently) = True              neg : pos    =      9.7 : 1.0\n",
      "      contains(tiresome) = True              neg : pos    =      9.7 : 1.0\n",
      "           contains(wry) = True              pos : neg    =      9.7 : 1.0\n",
      "      contains(powerful) = True              pos : neg    =      9.7 : 1.0\n",
      "        contains(nicely) = True              pos : neg    =      9.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = NOT_featuresets[200:], NOT_featuresets[:200]\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier1, test_set))\n",
    "classifier1.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7673545966228893\n",
      "mean accuracy 0.07673545966228892\n",
      "1 0.7729831144465291\n",
      "mean accuracy 0.15403377110694186\n",
      "2 0.7945590994371482\n",
      "mean accuracy 0.23348968105065668\n",
      "3 0.7532833020637899\n",
      "mean accuracy 0.3088180112570357\n",
      "4 0.7786116322701688\n",
      "mean accuracy 0.3866791744840526\n",
      "5 0.798311444652908\n",
      "mean accuracy 0.46651031894934336\n",
      "6 0.7851782363977486\n",
      "mean accuracy 0.5450281425891182\n",
      "7 0.7692307692307693\n",
      "mean accuracy 0.6219512195121951\n",
      "8 0.7701688555347092\n",
      "mean accuracy 0.698968105065666\n",
      "9 0.7917448405253283\n",
      "mean accuracy 0.7781425891181988\n"
     ]
    }
   ],
   "source": [
    "cross_validation(10, NOT_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cross validation, we can see that the accuracy of negation feature is about 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. POS Tag Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some classification tasks where part-of-speech tag features can have an effect. this is more likely for shorter units of classification, such as sentence level classification or shorter social media such as tweets.\n",
    "\n",
    "The most common way to use POS tagging information is to include counts of various types of word tags.\n",
    "\n",
    "Here is the definition of our new feature function, adding POS tag counts to the word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_features(document,word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = POS_featuresets[1000:], POS_featuresets[:1000]\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier2, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7560975609756098\n",
      "mean accuracy 0.07560975609756097\n",
      "1 0.7514071294559099\n",
      "mean accuracy 0.15075046904315198\n",
      "2 0.7439024390243902\n",
      "mean accuracy 0.225140712945591\n",
      "3 0.726078799249531\n",
      "mean accuracy 0.2977485928705441\n",
      "4 0.7720450281425891\n",
      "mean accuracy 0.374953095684803\n",
      "5 0.7833020637898687\n",
      "mean accuracy 0.45328330206378986\n",
      "6 0.7345215759849906\n",
      "mean accuracy 0.5267354596622889\n",
      "7 0.7326454033771107\n",
      "mean accuracy 0.6\n",
      "8 0.7288930581613509\n",
      "mean accuracy 0.672889305816135\n",
      "9 0.7298311444652908\n",
      "mean accuracy 0.7458724202626642\n"
     ]
    }
   ],
   "source": [
    "cross_validation(10, POS_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cross validation, we can see that the accuracy of negation feature is about 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bigram Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we worked on generating bigrams from documents, if we want to use highly frequent bigrams, we need to filter out special characters, which were very frequent in the bigrams, and also filter by frequency.  The bigram pmi measure also required some filtering to get frequent and meaningful bigrams.  \n",
    "\n",
    "But there is another bigram association measure that is more often used to filter bigrams for classification features.  This is the chi-squared measure, which is another measure of information gain, but which does its own frequency filtering.  Another frequently used alternative is to just use frequency, which is the bigram measure raw_freq.\n",
    "\n",
    "We’ll start by importing the collocations package and creating a short cut variable name for the bigram association measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a bigram collocation finder using the original movie review words, since the bigram finder must have the words in order.  Note that our all_words_list has exactly this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(all_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the chi-squared measure to get bigrams that are informative features.  Note that we don’t need to get the scores of the bigrams, so we use the nbest function which just returns the highest scoring bigrams, using the number specified.\n",
    "(Or try bigram_measures.raw_freq.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nbest function returns a list of significant bigrams in this corpus, and we can look at some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"''independent\", \"film''\"), (\"'60s-homage\", 'pokepie'), (\"'[the\", 'cockettes]'), (\"'ace\", \"ventura'\"), (\"'alternate\", \"reality'\"), (\"'aunque\", 'recurre'), (\"'black\", \"culture'\"), (\"'blue\", \"crush'\"), (\"'chan\", \"moment'\"), (\"'chick\", \"flicks'\"), (\"'date\", \"movie'\"), (\"'ethnic\", 'cleansing'), (\"'face\", \"value'\"), (\"'fully\", \"experienced'\"), (\"'hannibal'\", 'lauren'), (\"'jason\", \"x'\"), (\"'juvenile\", \"delinquent'\"), (\"'laugh\", \"therapy'\"), (\"'masterpiece\", \"theatre'\"), (\"'nicholas\", \"nickleby'\"), (\"'old\", \"neighborhood'\"), (\"'opening\", \"up'\"), (\"'rare\", \"birds'\"), (\"'sacre\", 'bleu'), (\"'science\", \"fiction'\"), (\"'shindler's\", \"list'\"), (\"'snow\", \"dogs'\"), (\"'some\", \"body'\"), (\"'special\", \"effects'\"), (\"'terrible\", \"filmmaking'\"), (\"'time\", \"waster'\"), (\"'true\", \"story'\"), (\"'unfaithful'\", 'cheats'), (\"'very\", \"sneaky'\"), (\"'we're\", '-doing-it-for'), (\"'who's\", \"who'\"), ('-after', 'spangle'), ('-as-it-', 'thinks-it-is'), ('-as-nasty', '-as-it-'), ('-doing-it-for', \"-the-cash'\"), ('10-course', 'banquet'), ('10-year', 'delay'), ('15-cent', 'stump'), ('18-year-old', 'mistress'), (\"1950's\", 'doris'), (\"1983's\", 'koyaanisqatsi'), ('1986', 'harlem'), (\"1988's\", 'powaqqatsi'), ('1992', 'malfitano-domingo'), (\"1992's\", 'unforgiven')]\n"
     ]
    }
   ],
   "source": [
    "print(bigram_features[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a feature extraction function that has all the word features as before, but also has bigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d,c) in documents]\n",
    "#There should be 2000 features:  1500 word features and 500 bigram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "classifier3 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier3, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.74812382739212\n",
      "mean accuracy 0.14962476547842402\n",
      "1 0.7429643527204502\n",
      "mean accuracy 0.29821763602251405\n",
      "2 0.7696998123827392\n",
      "mean accuracy 0.4521575984990619\n",
      "3 0.7274859287054409\n",
      "mean accuracy 0.5976547842401502\n",
      "4 0.7429643527204502\n",
      "mean accuracy 0.7462476547842403\n"
     ]
    }
   ],
   "source": [
    "cross_validation(5, bigram_featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cross validation, we can see that the accuracy of negation feature is about 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, adding negation features can lead to the highest accuracy. Thus, we will use negation features in our sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we imported the packaged we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                 \n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will pre-process our data.\n",
    "For data preprocessing, we need to tokenize our text. For our dataset, the review text are stored in rows of a dataframe, so we can just extract them from the dataframe and store it into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>Very pleased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                       Product Name Brand Name  \\\n",
       "0      0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "1      1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "2      2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "3      3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "4      4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "\n",
       "    Price                                            Reviews  \n",
       "0  199.99  I feel so LUCKY to have found this used (phone...  \n",
       "1  199.99  nice phone, nice up grade from my pantach revu...  \n",
       "2  199.99                                       Very pleased  \n",
       "3  199.99  It works good but it goes slow sometimes but i...  \n",
       "4  199.99  Great phone to replace my lost phone. The only...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.rename(columns={'Unnamed: 0':'Index'},inplace=True)\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I feel so LUCKY to have found this used (phone to us & not used hard at all), phone on line from someone who upgraded and sold this one. My Son liked his old one that finally fell apart after 2.5+ years and didn't want an upgrade!! Thank you Seller, we really appreciate it & your honesty re: said used phone.I recommend this seller very highly & would but from them again!!\",\n",
       " 'nice phone, nice up grade from my pantach revue. Very clean set up and easy set up. never had an android phone but they are fantastic to say the least. perfect size for surfing and social media. great phone samsung',\n",
       " 'Very pleased',\n",
       " 'It works good but it goes slow sometimes but its a very good phone I love it',\n",
       " 'Great phone to replace my lost phone. The only thing is the volume up button does not work, but I can still go into settings to adjust. Other than that, it does the job until I am eligible to upgrade my phone again.Thaanks!',\n",
       " 'I already had a phone with problems... I know it stated it was used, but dang, it did not state that it did not charge. I wish I would have read these comments then I would have not purchased this item.... and its cracked on the side.. damaged goods is what it is.... If trying to charge it another way does not work I am requesting for my money back... AND I WILL GET MY MONEY BACK...SIGNED AN UNHAPPY CUSTOMER....']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReviewList = df['Reviews'].tolist()\n",
    "ReviewList[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63038"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ReviewList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data preprocessing, we used the model with highest accuracy to find out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in ReviewList:\n",
    "    texttokens = nltk.word_tokenize(i)\n",
    "    textlower = [w.lower() for w in texttokens]\n",
    "    inputfeatureset = NOT_features(textlower, word_features, negationwords)\n",
    "    a = classifier1.classify(inputfeatureset)\n",
    "    label.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(list1, list2): \n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentList = merge(ReviewList, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_neg = []\n",
    "sentiment_pos = []\n",
    "Sentiment = []\n",
    "for i in sentimentList:\n",
    "    if i[1] == 'neg':\n",
    "        sentiment_neg.append(i[0])\n",
    "        Sentiment.append(int(0))\n",
    "    else:\n",
    "        sentiment_pos.append(i[0])\n",
    "        Sentiment.append(int(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44544"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18494"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63038"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                       Product Name Brand Name  \\\n",
       "0      0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "1      1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "2      2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "3      3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "4      4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung   \n",
       "\n",
       "    Price                                            Reviews  Sentiment  \n",
       "0  199.99  I feel so LUCKY to have found this used (phone...          0  \n",
       "1  199.99  nice phone, nice up grade from my pantach revu...          1  \n",
       "2  199.99                                       Very pleased          1  \n",
       "3  199.99  It works good but it goes slow sometimes but i...          0  \n",
       "4  199.99  Great phone to replace my lost phone. The only...          1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Samsung\n",
       "1    Samsung\n",
       "2    Samsung\n",
       "3    Samsung\n",
       "4    Samsung\n",
       "Name: Brand Name, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BrandList = df['Brand Name']\n",
    "BrandList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63033    79.95\n",
       "63034    79.95\n",
       "63035    79.95\n",
       "63036    79.95\n",
       "63037    79.95\n",
       "Name: Price, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pricelist = df['Price']\n",
    "Pricelist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D700*FRONT CAMERA*ANDROID*SLIDER*QWERTY KEYBOARD*TOUCH SCREEN',\n",
       "       'Cricket Samsung Galaxy Discover R740 Phone',\n",
       "       'Galaxy s III mini SM-G730V Verizon Cell Phone BLUE',\n",
       "       'Galaxy S5 G900A Factory Unlocked Android Smartphone 16GB White',\n",
       "       'Galaxy S6 Active - Camo Blue (Unlocked)',\n",
       "       'GreatCall Samsung Jitterbug Touch3 Senior Smartphone with 1-Touch Medical Alert and Large Display'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProductList = df['Product Name'].unique()\n",
    "ProductList[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the \"product score\" as the percentage of positive reviews. Since we defined positive review as 1 and negative review as 0, the calculation of percentage of positive reviews is the same with the caculation of the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ProductScore = []\n",
    "for i in range(0,len(ProductList)):\n",
    "    ProductScore.append(np.mean(df.loc[df['Product Name']==str(ProductList[i])]['Sentiment'].tolist()))\n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2702702702702703,\n",
       " 0.2222222222222222,\n",
       " 0.0,\n",
       " 0.30125,\n",
       " 0.0,\n",
       " 0.24161073825503357,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.28846153846153844,\n",
       " 0.15217391304347827,\n",
       " 0.3157894736842105,\n",
       " 0.0,\n",
       " 0.21212121212121213,\n",
       " 0.2222222222222222,\n",
       " 0.09090909090909091,\n",
       " 0.375,\n",
       " 0.43333333333333335,\n",
       " 0.15217391304347827,\n",
       " 0.45454545454545453,\n",
       " 0.4375,\n",
       " 0.0,\n",
       " 0.15,\n",
       " 0.06666666666666667,\n",
       " 0.24561403508771928,\n",
       " 0.14285714285714285,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.16666666666666666,\n",
       " 0.1724137931034483,\n",
       " 0.30434782608695654,\n",
       " 0.36363636363636365,\n",
       " 0.24324324324324326,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.2,\n",
       " 0.5,\n",
       " 0.23529411764705882,\n",
       " 0.21621621621621623,\n",
       " 0.2857142857142857,\n",
       " 0.21212121212121213,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.14772727272727273,\n",
       " 0.0,\n",
       " 0.39344262295081966,\n",
       " 0.2598870056497175,\n",
       " 0.26666666666666666,\n",
       " 0.19642857142857142,\n",
       " 0.3684210526315789,\n",
       " 0.8,\n",
       " 0.35135135135135137,\n",
       " 0.28,\n",
       " 0.28,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.3333333333333333,\n",
       " 0.18181818181818182,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.18181818181818182,\n",
       " 0.3464566929133858,\n",
       " 0.38461538461538464,\n",
       " 0.0,\n",
       " 0.26666666666666666,\n",
       " 0.2222222222222222,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4444444444444444,\n",
       " 0.3142857142857143,\n",
       " 0.2827586206896552,\n",
       " 0.0,\n",
       " 0.3054545454545455,\n",
       " 0.3247422680412371,\n",
       " 0.4318181818181818,\n",
       " 0.35135135135135137,\n",
       " 0.35135135135135137,\n",
       " 0.375,\n",
       " 0.25,\n",
       " 0.26881720430107525,\n",
       " 0.0,\n",
       " 0.13166855845629966,\n",
       " 0.125,\n",
       " 0.5,\n",
       " 0.42857142857142855,\n",
       " 0.2727272727272727,\n",
       " 0.391304347826087,\n",
       " 0.2907801418439716,\n",
       " 0.0,\n",
       " 0.35499398315282793,\n",
       " 1.0,\n",
       " 0.271523178807947,\n",
       " 0.3088235294117647,\n",
       " 0.25773195876288657,\n",
       " 0.2664714494875549,\n",
       " 0.25068493150684934,\n",
       " 0.29411764705882354,\n",
       " 0.3125,\n",
       " 0.29150326797385623,\n",
       " 0.1864406779661017,\n",
       " 0.6666666666666666,\n",
       " 0.2,\n",
       " 0.3333333333333333,\n",
       " 0.2905982905982906,\n",
       " 0.2916666666666667,\n",
       " 0.2916666666666667,\n",
       " 0.42857142857142855,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1724137931034483,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.41379310344827586,\n",
       " 0.38461538461538464,\n",
       " 0.375,\n",
       " 0.23076923076923078,\n",
       " 0.2830188679245283,\n",
       " 0.27450980392156865,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.25,\n",
       " 0.2727272727272727,\n",
       " 0.3125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3125,\n",
       " 0.16666666666666666,\n",
       " 0.14285714285714285,\n",
       " 0.2463768115942029,\n",
       " 0.2625,\n",
       " 0.3333333333333333,\n",
       " 0.34108527131782945,\n",
       " 0.3670886075949367,\n",
       " 0.23076923076923078,\n",
       " 0.2857142857142857,\n",
       " 0.23076923076923078,\n",
       " 0.35732647814910024,\n",
       " 0.32598039215686275,\n",
       " 0.3501199040767386,\n",
       " 0.3972602739726027,\n",
       " 0.37583892617449666,\n",
       " 0.35542168674698793,\n",
       " 0.37209302325581395,\n",
       " 0.2647058823529412,\n",
       " 0.0,\n",
       " 0.33472803347280333,\n",
       " 0.3523809523809524,\n",
       " 0.33962264150943394,\n",
       " 0.3311827956989247,\n",
       " 0.29559748427672955,\n",
       " 0.376984126984127,\n",
       " 0.26666666666666666,\n",
       " 0.25,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.29411764705882354,\n",
       " 0.3048128342245989,\n",
       " 0.4166666666666667,\n",
       " 0.29411764705882354,\n",
       " 0.27702702702702703,\n",
       " 0.29411764705882354,\n",
       " 0.3333333333333333,\n",
       " 0.2603305785123967,\n",
       " 0.46153846153846156,\n",
       " 0.2786885245901639,\n",
       " 0.29411764705882354,\n",
       " 0.29545454545454547,\n",
       " 0.3148148148148148,\n",
       " 0.2578397212543554,\n",
       " 0.254,\n",
       " 0.35714285714285715,\n",
       " 0.28308823529411764,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.4444444444444444,\n",
       " 0.3333333333333333,\n",
       " 0.340632603406326,\n",
       " 0.0,\n",
       " 0.23529411764705882,\n",
       " 0.26,\n",
       " 0.3197278911564626,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.2,\n",
       " 0.43478260869565216,\n",
       " 0.5714285714285714,\n",
       " 0.6,\n",
       " 0.25,\n",
       " 0.23668639053254437,\n",
       " 0.22966507177033493,\n",
       " 0.226890756302521,\n",
       " 0.35684647302904565,\n",
       " 0.3741935483870968,\n",
       " 0.3388704318936877,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.32732732732732733,\n",
       " 0.21348314606741572,\n",
       " 0.22826086956521738,\n",
       " 0.17733990147783252,\n",
       " 0.25,\n",
       " 0.3153846153846154,\n",
       " 0.30612244897959184,\n",
       " 0.0,\n",
       " 0.39416058394160586,\n",
       " 0.29372937293729373,\n",
       " 0.25,\n",
       " 0.4074074074074074,\n",
       " 0.5,\n",
       " 0.4444444444444444,\n",
       " 0.2680851063829787,\n",
       " 0.1724137931034483,\n",
       " 0.26495726495726496,\n",
       " 0.6666666666666666,\n",
       " 0.4444444444444444,\n",
       " 0.45098039215686275,\n",
       " 0.4318181818181818,\n",
       " 0.36363636363636365,\n",
       " 0.36363636363636365,\n",
       " 0.3333333333333333,\n",
       " 0.3783783783783784,\n",
       " 0.5,\n",
       " 0.31952662721893493,\n",
       " 0.22580645161290322,\n",
       " 0.5,\n",
       " 0.23809523809523808,\n",
       " 0.23333333333333334,\n",
       " 0.18246445497630331,\n",
       " 0.42857142857142855,\n",
       " 0.3333333333333333,\n",
       " 0.2996389891696751,\n",
       " 0.3710691823899371,\n",
       " 0.14285714285714285,\n",
       " 0.3118279569892473,\n",
       " 0.0,\n",
       " 0.2727272727272727,\n",
       " 0.3333333333333333,\n",
       " 0.24079320113314448,\n",
       " 0.32656826568265684,\n",
       " 0.18604651162790697,\n",
       " 0.15306122448979592,\n",
       " 0.22727272727272727,\n",
       " 0.0,\n",
       " 0.5714285714285714,\n",
       " 0.16666666666666666,\n",
       " 0.22425249169435216,\n",
       " 0.23648648648648649,\n",
       " 0.0,\n",
       " 0.22608695652173913,\n",
       " 0.125,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.22088353413654618,\n",
       " 0.3,\n",
       " 0.25862068965517243,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.3333333333333333,\n",
       " 0.31693989071038253,\n",
       " 0.37375745526838966,\n",
       " 0.32123655913978494,\n",
       " 0.2,\n",
       " 0.3,\n",
       " 0.3420074349442379,\n",
       " 0.34615384615384615,\n",
       " 0.2925531914893617,\n",
       " 0.0,\n",
       " 0.25190839694656486,\n",
       " 0.23940435280641467,\n",
       " 0.3333333333333333,\n",
       " 0.3076923076923077,\n",
       " 0.3,\n",
       " 0.3333333333333333,\n",
       " 0.16853932584269662,\n",
       " 0.24489795918367346,\n",
       " 0.37254901960784315,\n",
       " 0.4375,\n",
       " 0.0,\n",
       " 0.3389830508474576,\n",
       " 0.27185398655139287,\n",
       " 0.3157894736842105,\n",
       " 0.26582278481012656,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.1927710843373494,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.2,\n",
       " 0.15,\n",
       " 0.45454545454545453,\n",
       " 0.2992,\n",
       " 0.3132530120481928,\n",
       " 0.2903225806451613,\n",
       " 0.29545454545454547,\n",
       " 0.3404255319148936,\n",
       " 0.19767441860465115,\n",
       " 0.2785114045618247,\n",
       " 0.2773279352226721,\n",
       " 0.23728813559322035,\n",
       " 0.2992125984251969,\n",
       " 0.23076923076923078,\n",
       " 0.0,\n",
       " 0.2222222222222222,\n",
       " 0.3006134969325153,\n",
       " 0.0,\n",
       " 0.46153846153846156,\n",
       " 0.2,\n",
       " 0.2727272727272727,\n",
       " 0.24434389140271492,\n",
       " 0.0,\n",
       " 0.343042071197411,\n",
       " 0.3291139240506329,\n",
       " 0.1,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.19662921348314608,\n",
       " 0.23529411764705882,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.3197389885807504,\n",
       " 0.33648393194706994,\n",
       " 0.34375,\n",
       " 0.35546875,\n",
       " 0.353757225433526,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.28625,\n",
       " 0.27472527472527475,\n",
       " 0.2988505747126437,\n",
       " 0.3125,\n",
       " 0.3414141414141414,\n",
       " 0.1,\n",
       " 0.25862068965517243,\n",
       " 0.24262295081967214,\n",
       " 0.2,\n",
       " 0.36,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.2483221476510067,\n",
       " 0.21428571428571427,\n",
       " 0.3235294117647059,\n",
       " 0.2750455373406193,\n",
       " 0.19,\n",
       " 0.2222222222222222,\n",
       " 0.2375,\n",
       " 0.20833333333333334,\n",
       " 0.4230769230769231,\n",
       " 0.14285714285714285,\n",
       " 0.24,\n",
       " 0.2,\n",
       " 0.16666666666666666,\n",
       " 0.315136476426799,\n",
       " 0.30434782608695654,\n",
       " 0.2839506172839506,\n",
       " 0.27692307692307694,\n",
       " 0.35172413793103446,\n",
       " 0.3145539906103286,\n",
       " 0.5,\n",
       " 0.5555555555555556,\n",
       " 0.5555555555555556,\n",
       " 0.5555555555555556,\n",
       " 0.0,\n",
       " 0.3518987341772152,\n",
       " 0.32727272727272727,\n",
       " 0.36363636363636365,\n",
       " 0.125,\n",
       " 0.34564643799472294,\n",
       " 0.5714285714285714,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.3473282442748092,\n",
       " 0.25,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.36363636363636365,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.3547008547008547,\n",
       " 0.0,\n",
       " 0.18181818181818182,\n",
       " 0.33587786259541985,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3515151515151515,\n",
       " 0.5555555555555556,\n",
       " 0.23809523809523808,\n",
       " 0.23529411764705882,\n",
       " 0.3333333333333333,\n",
       " 0.36019736842105265,\n",
       " 0.34523809523809523,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25748502994011974,\n",
       " 0.16666666666666666,\n",
       " 0.37037037037037035,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3978494623655914,\n",
       " 0.3611111111111111,\n",
       " 0.35106382978723405,\n",
       " 0.34234234234234234,\n",
       " 0.358974358974359,\n",
       " 0.26353276353276356,\n",
       " 0.4166666666666667,\n",
       " 0.3188405797101449,\n",
       " 0.38372093023255816,\n",
       " 0.3125,\n",
       " 0.75,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.2222222222222222,\n",
       " 0.371900826446281,\n",
       " 0.36200716845878134,\n",
       " 0.4307692307692308,\n",
       " 0.3613707165109034,\n",
       " 0.5238095238095238,\n",
       " 0.3923913043478261,\n",
       " 0.3333333333333333,\n",
       " 0.5,\n",
       " 0.4307692307692308,\n",
       " 0.3793103448275862,\n",
       " 0.32786885245901637,\n",
       " 0.37583892617449666,\n",
       " 0.3302752293577982,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.38461538461538464,\n",
       " 0.26666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13636363636363635,\n",
       " 0.17647058823529413,\n",
       " 0.17647058823529413,\n",
       " 0.17647058823529413,\n",
       " 0.3150684931506849,\n",
       " 0.16666666666666666,\n",
       " 0.358695652173913,\n",
       " 0.3684210526315789,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.2558139534883721,\n",
       " 0.35714285714285715,\n",
       " 0.2,\n",
       " 0.14,\n",
       " 0.25,\n",
       " 0.34054054054054056,\n",
       " 0.36363636363636365,\n",
       " 1.0,\n",
       " 0.24193548387096775,\n",
       " 0.22857142857142856,\n",
       " 0.47619047619047616,\n",
       " 0.5454545454545454,\n",
       " 0.2529411764705882,\n",
       " 0.4,\n",
       " 0.3132530120481928,\n",
       " 0.38461538461538464,\n",
       " 0.17073170731707318,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.22916666666666666,\n",
       " 0.45614035087719296,\n",
       " 0.24,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 0.28205128205128205,\n",
       " 0.2857142857142857,\n",
       " 0.36363636363636365,\n",
       " 0.34683098591549294,\n",
       " 1.0,\n",
       " 0.28,\n",
       " 0.2625,\n",
       " 0.34810126582278483,\n",
       " 0.2956989247311828,\n",
       " 0.4,\n",
       " 0.19444444444444445,\n",
       " 0.36,\n",
       " 0.0,\n",
       " 0.28,\n",
       " 0.2727272727272727,\n",
       " 0.16030534351145037,\n",
       " 0.0,\n",
       " 0.22580645161290322,\n",
       " 0.17,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.1875,\n",
       " 0.0,\n",
       " 0.13793103448275862,\n",
       " 0.25396825396825395,\n",
       " 0.3333333333333333,\n",
       " 0.2679738562091503,\n",
       " 0.3333333333333333,\n",
       " 0.32142857142857145,\n",
       " 0.2857142857142857,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.15625,\n",
       " 0.14942528735632185,\n",
       " 0.2222222222222222,\n",
       " 0.0,\n",
       " 0.42105263157894735,\n",
       " 0.19718309859154928,\n",
       " 0.3266509433962264,\n",
       " 0.5238095238095238,\n",
       " 0.15789473684210525,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.2222222222222222,\n",
       " 0.2571428571428571,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.6666666666666666,\n",
       " 0.3448275862068966,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.15384615384615385,\n",
       " 0.2222222222222222,\n",
       " 0.06896551724137931,\n",
       " 0.21052631578947367,\n",
       " 0.0,\n",
       " 0.38461538461538464,\n",
       " 0.3,\n",
       " 0.2608695652173913,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.13793103448275862,\n",
       " 0.14285714285714285,\n",
       " 0.3333333333333333,\n",
       " 0.2727272727272727,\n",
       " 0.2193877551020408,\n",
       " 0.2727272727272727,\n",
       " 0.14285714285714285,\n",
       " 0.13513513513513514,\n",
       " 0.19760479041916168,\n",
       " 0.20967741935483872,\n",
       " 0.23076923076923078,\n",
       " 0.18309859154929578,\n",
       " 0.35294117647058826,\n",
       " 0.21951219512195122,\n",
       " 0.5,\n",
       " 0.26666666666666666,\n",
       " 0.0,\n",
       " 0.2737430167597765,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.17647058823529413,\n",
       " 0.0,\n",
       " 0.4375,\n",
       " 0.3584905660377358,\n",
       " 0.3142857142857143,\n",
       " 0.2712765957446808,\n",
       " 0.22093023255813954,\n",
       " 0.21739130434782608,\n",
       " 0.19642857142857142,\n",
       " 0.2857142857142857]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProductScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a new data frame called 'ScoreDf'. \n",
    "We stored the product name and their percentage of positive reviews in this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreDf = pd.DataFrame(columns=['Product Name','Brand Name','Price','Positive Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreDf['Product Name']=ProductList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreDf['Price']=Pricelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreDf['Brand Name']=BrandList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreDf['Positive Percentage']=ProductScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Positive Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>0.270270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Cricket Samsung Galaxy Discover R740 Phone</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Galaxy s III mini SM-G730V Verizon Cell Phone ...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Galaxy S5 G900A Factory Unlocked Android Smart...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>0.301250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Galaxy S6 Active - Camo Blue (Unlocked)</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1         Cricket Samsung Galaxy Discover R740 Phone    Samsung  199.99   \n",
       "2  Galaxy s III mini SM-G730V Verizon Cell Phone ...    Samsung  199.99   \n",
       "3  Galaxy S5 G900A Factory Unlocked Android Smart...    Samsung  199.99   \n",
       "4            Galaxy S6 Active - Camo Blue (Unlocked)    Samsung  199.99   \n",
       "\n",
       "   Positive Percentage  \n",
       "0             0.270270  \n",
       "1             0.222222  \n",
       "2             0.000000  \n",
       "3             0.301250  \n",
       "4             0.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ScoreDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Positive Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>Verizon Samsung Alias 2 U750 No Contract Dual-...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0.271277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>Verizon Samsung Convoy U640 No Contract Milita...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0.220930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>Verizon Wireless Cell Phone Samsung Gusto U360...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>VERIZON WIRELESS CELL PHONE SAMSUNG U460 INTEN...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0.196429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>Samsung Convoy U640 Phone for Verizon Wireless...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Product Name Brand Name  Price  \\\n",
       "569  Verizon Samsung Alias 2 U750 No Contract Dual-...    Samsung  222.0   \n",
       "570  Verizon Samsung Convoy U640 No Contract Milita...    Samsung  222.0   \n",
       "571  Verizon Wireless Cell Phone Samsung Gusto U360...    Samsung  222.0   \n",
       "572  VERIZON WIRELESS CELL PHONE SAMSUNG U460 INTEN...    Samsung  222.0   \n",
       "573  Samsung Convoy U640 Phone for Verizon Wireless...    Samsung  222.0   \n",
       "\n",
       "     Positive Percentage  \n",
       "569             0.271277  \n",
       "570             0.220930  \n",
       "571             0.217391  \n",
       "572             0.196429  \n",
       "573             0.285714  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ScoreDf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreDf.to_csv('Percent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreDf1 = pd.DataFrame(columns=['Positive Percentage'])\n",
    "ScoreDf1['Positive Percentage']=ProductScore\n",
    "ScoreDf1.to_csv('score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "priceDf1 = pd.DataFrame(columns=['Price'])\n",
    "priceDf1['Price']=Pricelist\n",
    "priceDf1.to_csv('price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandDf1 = pd.DataFrame(columns=['Brand Name'])\n",
    "brandDf1['Brand Name']=BrandList\n",
    "brandDf1.to_csv('brand.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDf1 = pd.DataFrame(columns=['Product Name'])\n",
    "productDf1['Product Name']=ProductList\n",
    "productDf1.to_csv('product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
